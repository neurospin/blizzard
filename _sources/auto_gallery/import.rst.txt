

.. _sphx_glr_auto_gallery_import.py:


NeuroSpin file importer
=======================

Credit: A Grigis

In order to test if the 'blizzard' package is installed on your machine,
you can check the package version.



.. code-block:: python


    import os
    import json
    import glob
    from datetime import datetime
    import blizzard
    from blizzard import Blizzard
    print(blizzard.__version__)


Now we can define the ElasticSearch mappings for each index.



.. code-block:: python


    content_mapping = {
        "mappings": {
            "properties": {
                "path": {"type": "keyword"},
                "ls": {"type": "text"}
            }
        }
    }
    timestamp_mapping = {
        "mappings": {
            "properties": {
                "project": {"type": "keyword"},
                "timestamp": {"type": "text"}
            }
        }
    }
    files_mapping = {
        "mappings": {
            "properties": {
                "path": {"type": "keyword"},
                "keys": {"type": "text"}
            }
        }
    }


Now we create a connection to the ElasticSearch service and delete the
existing index.



.. code-block:: python


    importer = Blizzard(url="https://kratos.intra.cea.fr", verify_certs=False)
    importer.delete_index("content")
    importer.delete_index("files")
    importer.delete_index("timestamp")


We then create the index mappings.



.. code-block:: python


    importer.add_mapping("content", content_mapping)
    importer.add_mapping("files", files_mapping)
    importer.add_mapping("timestamp", timestamp_mapping)


And finally index the files. Note that some files have been splited in order
not to reach the POST size limitation. You may have to change the
'file_meta_dir'.



.. code-block:: python


    file_meta_dir = "/neurospin/nsap/processed/elasticfs/data"
    timestamps = []
    actions = []
    for study in os.listdir(file_meta_dir):
        print("Importing {0}...".format(study))
        parts = glob.glob(os.path.join(
            file_meta_dir, study, "{0}_bulk_part*.json".format(study)))
        mtime = datetime.fromtimestamp(os.path.getmtime(parts[0])).strftime(
            "%d/%m/%Y")
        timestamps.append({
            "_op_type": "create",
            "_index": "timestamp",
            "_id": study,
            "_source": {
                "project": study,
                "timestamp": mtime
            }
        })
        actions.extend(parts)
    importer.import_data(timestamps)
    for actions_file in actions:
        print("Processing '{0}'...".format(actions_file))
        with open(actions_file, "rt") as open_file:
            actions = json.load(open_file)
        importer.import_data(actions)

**Total running time of the script:** ( 0 minutes  0.000 seconds)



.. only :: html

 .. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: import.py <import.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: import.ipynb <import.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    Gallery generated by Sphinx-Gallery


